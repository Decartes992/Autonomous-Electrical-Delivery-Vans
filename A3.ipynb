{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Decartes992/Autonomous-Electrical-Delivery-Vans/blob/main/A3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zq08FzG6siOV"
      },
      "source": [
        "# Assignment 3: Deep Learning Using Keras\n",
        "**Iftekhar Rafi B00871031**\n",
        "\n",
        "**Abdulla Sadoun B00900541**\n",
        "\n",
        "## Question 0: Revisiting Your Dataset (A1Q1)\n",
        "## Dataset Loading and Initial Exploration\n",
        "\n",
        "### a) Dataset Selection\n",
        "For this assignment, We have reselected the **CSE-CIC-IDS2018** dataset from the **Canadian Institute for Cybersecurity at the University of New Brunswick** also used for Assignment 1. This dataset is well-suited for analyzing **network intrusion detection**. Several methods of network intrusion have been explored in this research and subsequent dataset. I will be focusing on the dataset collected from **brute-force attack scenarios** as described in the research. The data is taken from their processed dataset for Wednesday, 14 February as described below.\n",
        "\n",
        "#### Dataset Details\n",
        "- **Source**: [CICIDS 2018 Dataset](https://www.unb.ca/cic/datasets/ids-2018.html)\n",
        "- **Types of Attacks Covered**: Brute-force attacks (FTP and SSH)\n",
        "\n",
        "| Attacker                     | Victim                          | Attack Name      | Date          | Attack Start Time | Attack Finish Time |\n",
        "|------------------------------|---------------------------------|------------------|---------------|-------------------|--------------------|\n",
        "| 172.31.70.4 (Valid IP:18.221.219.4) | 172.31.69.25 (Valid IP:18.217.21.148) | FTP-BruteForce   | Wed-14-02-2018 | 10:32             | 12:09              |\n",
        "| 172.31.70.6 (Valid IP:13.58.98.64)  | 18.217.21.148- 172.31.69.25          | SSH-BruteForce   | Wed-14-02-2018 | 14:01             | 15:31              |\n",
        "\n",
        "\n",
        "### b) Dataset Loading\n",
        "The dataset was loaded into a Pandas DataFrame using the following code:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "PXmSyWNMsiOW",
        "outputId": "9ac282de-02d7-42a6-d962-cfadcb96bf50"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1048575 entries, 0 to 1048574\n",
            "Data columns (total 80 columns):\n",
            " #   Column             Non-Null Count    Dtype  \n",
            "---  ------             --------------    -----  \n",
            " 0   Dst Port           1048575 non-null  int64  \n",
            " 1   Protocol           1048575 non-null  int64  \n",
            " 2   Timestamp          1048575 non-null  object \n",
            " 3   Flow Duration      1048575 non-null  int64  \n",
            " 4   Tot Fwd Pkts       1048575 non-null  int64  \n",
            " 5   Tot Bwd Pkts       1048575 non-null  int64  \n",
            " 6   TotLen Fwd Pkts    1048575 non-null  int64  \n",
            " 7   TotLen Bwd Pkts    1048575 non-null  int64  \n",
            " 8   Fwd Pkt Len Max    1048575 non-null  int64  \n",
            " 9   Fwd Pkt Len Min    1048575 non-null  int64  \n",
            " 10  Fwd Pkt Len Mean   1048575 non-null  float64\n",
            " 11  Fwd Pkt Len Std    1048575 non-null  float64\n",
            " 12  Bwd Pkt Len Max    1048575 non-null  int64  \n",
            " 13  Bwd Pkt Len Min    1048575 non-null  int64  \n",
            " 14  Bwd Pkt Len Mean   1048575 non-null  float64\n",
            " 15  Bwd Pkt Len Std    1048575 non-null  float64\n",
            " 16  Flow Byts/s        1046298 non-null  float64\n",
            " 17  Flow Pkts/s        1048575 non-null  float64\n",
            " 18  Flow IAT Mean      1048575 non-null  float64\n",
            " 19  Flow IAT Std       1048575 non-null  float64\n",
            " 20  Flow IAT Max       1048575 non-null  int64  \n",
            " 21  Flow IAT Min       1048575 non-null  int64  \n",
            " 22  Fwd IAT Tot        1048575 non-null  int64  \n",
            " 23  Fwd IAT Mean       1048575 non-null  float64\n",
            " 24  Fwd IAT Std        1048575 non-null  float64\n",
            " 25  Fwd IAT Max        1048575 non-null  int64  \n",
            " 26  Fwd IAT Min        1048575 non-null  int64  \n",
            " 27  Bwd IAT Tot        1048575 non-null  int64  \n",
            " 28  Bwd IAT Mean       1048575 non-null  float64\n",
            " 29  Bwd IAT Std        1048575 non-null  float64\n",
            " 30  Bwd IAT Max        1048575 non-null  int64  \n",
            " 31  Bwd IAT Min        1048575 non-null  int64  \n",
            " 32  Fwd PSH Flags      1048575 non-null  int64  \n",
            " 33  Bwd PSH Flags      1048575 non-null  int64  \n",
            " 34  Fwd URG Flags      1048575 non-null  int64  \n",
            " 35  Bwd URG Flags      1048575 non-null  int64  \n",
            " 36  Fwd Header Len     1048575 non-null  int64  \n",
            " 37  Bwd Header Len     1048575 non-null  int64  \n",
            " 38  Fwd Pkts/s         1048575 non-null  float64\n",
            " 39  Bwd Pkts/s         1048575 non-null  float64\n",
            " 40  Pkt Len Min        1048575 non-null  int64  \n",
            " 41  Pkt Len Max        1048575 non-null  int64  \n",
            " 42  Pkt Len Mean       1048575 non-null  float64\n",
            " 43  Pkt Len Std        1048575 non-null  float64\n",
            " 44  Pkt Len Var        1048575 non-null  float64\n",
            " 45  FIN Flag Cnt       1048575 non-null  int64  \n",
            " 46  SYN Flag Cnt       1048575 non-null  int64  \n",
            " 47  RST Flag Cnt       1048575 non-null  int64  \n",
            " 48  PSH Flag Cnt       1048575 non-null  int64  \n",
            " 49  ACK Flag Cnt       1048575 non-null  int64  \n",
            " 50  URG Flag Cnt       1048575 non-null  int64  \n",
            " 51  CWE Flag Count     1048575 non-null  int64  \n",
            " 52  ECE Flag Cnt       1048575 non-null  int64  \n",
            " 53  Down/Up Ratio      1048575 non-null  int64  \n",
            " 54  Pkt Size Avg       1048575 non-null  float64\n",
            " 55  Fwd Seg Size Avg   1048575 non-null  float64\n",
            " 56  Bwd Seg Size Avg   1048575 non-null  float64\n",
            " 57  Fwd Byts/b Avg     1048575 non-null  int64  \n",
            " 58  Fwd Pkts/b Avg     1048575 non-null  int64  \n",
            " 59  Fwd Blk Rate Avg   1048575 non-null  int64  \n",
            " 60  Bwd Byts/b Avg     1048575 non-null  int64  \n",
            " 61  Bwd Pkts/b Avg     1048575 non-null  int64  \n",
            " 62  Bwd Blk Rate Avg   1048575 non-null  int64  \n",
            " 63  Subflow Fwd Pkts   1048575 non-null  int64  \n",
            " 64  Subflow Fwd Byts   1048575 non-null  int64  \n",
            " 65  Subflow Bwd Pkts   1048575 non-null  int64  \n",
            " 66  Subflow Bwd Byts   1048575 non-null  int64  \n",
            " 67  Init Fwd Win Byts  1048575 non-null  int64  \n",
            " 68  Init Bwd Win Byts  1048575 non-null  int64  \n",
            " 69  Fwd Act Data Pkts  1048575 non-null  int64  \n",
            " 70  Fwd Seg Size Min   1048575 non-null  int64  \n",
            " 71  Active Mean        1048575 non-null  float64\n",
            " 72  Active Std         1048575 non-null  float64\n",
            " 73  Active Max         1048575 non-null  int64  \n",
            " 74  Active Min         1048575 non-null  int64  \n",
            " 75  Idle Mean          1048575 non-null  float64\n",
            " 76  Idle Std           1048575 non-null  float64\n",
            " 77  Idle Max           1048575 non-null  int64  \n",
            " 78  Idle Min           1048575 non-null  int64  \n",
            " 79  Label              1048575 non-null  object \n",
            "dtypes: float64(24), int64(54), object(2)\n",
            "memory usage: 640.0+ MB\n",
            "None\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Dst Port  Protocol            Timestamp  Flow Duration  Tot Fwd Pkts  \\\n",
              "0         0         0  14/02/2018 08:31:01      112641719             3   \n",
              "1         0         0  14/02/2018 08:33:50      112641466             3   \n",
              "2         0         0  14/02/2018 08:36:39      112638623             3   \n",
              "3        22         6  14/02/2018 08:40:13        6453966            15   \n",
              "4        22         6  14/02/2018 08:40:23        8804066            14   \n",
              "\n",
              "   Tot Bwd Pkts  TotLen Fwd Pkts  TotLen Bwd Pkts  Fwd Pkt Len Max  \\\n",
              "0             0                0                0                0   \n",
              "1             0                0                0                0   \n",
              "2             0                0                0                0   \n",
              "3            10             1239             2273              744   \n",
              "4            11             1143             2209              744   \n",
              "\n",
              "   Fwd Pkt Len Min  ...  Fwd Seg Size Min  Active Mean  Active Std  \\\n",
              "0                0  ...                 0          0.0         0.0   \n",
              "1                0  ...                 0          0.0         0.0   \n",
              "2                0  ...                 0          0.0         0.0   \n",
              "3                0  ...                32          0.0         0.0   \n",
              "4                0  ...                32          0.0         0.0   \n",
              "\n",
              "   Active Max  Active Min   Idle Mean    Idle Std  Idle Max  Idle Min   Label  \n",
              "0           0           0  56320859.5  139.300036  56320958  56320761  Benign  \n",
              "1           0           0  56320733.0  114.551299  56320814  56320652  Benign  \n",
              "2           0           0  56319311.5  301.934596  56319525  56319098  Benign  \n",
              "3           0           0         0.0    0.000000         0         0  Benign  \n",
              "4           0           0         0.0    0.000000         0         0  Benign  \n",
              "\n",
              "[5 rows x 80 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d40ac9ba-451b-4c82-ad1f-4b9739709f9d\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Dst Port</th>\n",
              "      <th>Protocol</th>\n",
              "      <th>Timestamp</th>\n",
              "      <th>Flow Duration</th>\n",
              "      <th>Tot Fwd Pkts</th>\n",
              "      <th>Tot Bwd Pkts</th>\n",
              "      <th>TotLen Fwd Pkts</th>\n",
              "      <th>TotLen Bwd Pkts</th>\n",
              "      <th>Fwd Pkt Len Max</th>\n",
              "      <th>Fwd Pkt Len Min</th>\n",
              "      <th>...</th>\n",
              "      <th>Fwd Seg Size Min</th>\n",
              "      <th>Active Mean</th>\n",
              "      <th>Active Std</th>\n",
              "      <th>Active Max</th>\n",
              "      <th>Active Min</th>\n",
              "      <th>Idle Mean</th>\n",
              "      <th>Idle Std</th>\n",
              "      <th>Idle Max</th>\n",
              "      <th>Idle Min</th>\n",
              "      <th>Label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>14/02/2018 08:31:01</td>\n",
              "      <td>112641719</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>56320859.5</td>\n",
              "      <td>139.300036</td>\n",
              "      <td>56320958</td>\n",
              "      <td>56320761</td>\n",
              "      <td>Benign</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>14/02/2018 08:33:50</td>\n",
              "      <td>112641466</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>56320733.0</td>\n",
              "      <td>114.551299</td>\n",
              "      <td>56320814</td>\n",
              "      <td>56320652</td>\n",
              "      <td>Benign</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>14/02/2018 08:36:39</td>\n",
              "      <td>112638623</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>56319311.5</td>\n",
              "      <td>301.934596</td>\n",
              "      <td>56319525</td>\n",
              "      <td>56319098</td>\n",
              "      <td>Benign</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>22</td>\n",
              "      <td>6</td>\n",
              "      <td>14/02/2018 08:40:13</td>\n",
              "      <td>6453966</td>\n",
              "      <td>15</td>\n",
              "      <td>10</td>\n",
              "      <td>1239</td>\n",
              "      <td>2273</td>\n",
              "      <td>744</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>32</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Benign</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>22</td>\n",
              "      <td>6</td>\n",
              "      <td>14/02/2018 08:40:23</td>\n",
              "      <td>8804066</td>\n",
              "      <td>14</td>\n",
              "      <td>11</td>\n",
              "      <td>1143</td>\n",
              "      <td>2209</td>\n",
              "      <td>744</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>32</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Benign</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 80 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d40ac9ba-451b-4c82-ad1f-4b9739709f9d')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-d40ac9ba-451b-4c82-ad1f-4b9739709f9d button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-d40ac9ba-451b-4c82-ad1f-4b9739709f9d');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-05858f4c-c802-455a-8179-3aa4163fa670\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-05858f4c-c802-455a-8179-3aa4163fa670')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-05858f4c-c802-455a-8179-3aa4163fa670 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Load the dataset\n",
        "file_path = '/content/drive/My Drive/Wednesday-14-02-2018_TrafficForML_CICFlowMeter.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Display basic information about the dataset\n",
        "print(df.info())\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pl_QHHw8siOX"
      },
      "source": [
        "### c) Description of the Dataset\n",
        "\n",
        "The dataset consists of network traffic data collected on **February 14, 2018**, as part of the **CSE-CIC-IDS2018** dataset. It captures various network activities, including normal (benign) traffic and brute-force attacks targeting FTP and SSH services. The dataset includes **1,048,575 rows** and **80 columns**, each representing different characteristics of the network flows.\n",
        "\n",
        "Here is a breakdown of the dataset:\n",
        "- **Total Number of Records**: 1,048,575\n",
        "- **Total Number of Features (Columns)**: 80\n",
        "- **Types of Data**:\n",
        "  - **Numeric Features**: Most of the columns are numeric, such as packet counts, flow durations, and packet sizes. These can help analyze the behavior of network traffic.\n",
        "  - **Categorical Features**: The dataset includes a column labeled \"Label,\" which identifies whether a given flow is benign or part of a specific attack type.\n",
        "\n",
        "### Key Features in the Dataset\n",
        "- **Flow Duration**: Measures how long a network flow lasted.\n",
        "- **Total Forward and Backward Packets** (`Tot Fwd Pkts`, `Tot Bwd Pkts`): Counts the number of packets sent in the forward and backward directions.\n",
        "- **Packet Length Statistics**: Provides information about the maximum, minimum, average, and standard deviation of packet lengths in both directions.\n",
        "- **Flow Rate (`Flow Byts/s`, `Flow Pkts/s`)**: Indicates the number of bytes or packets transmitted per second during a flow.\n",
        "- **Inter-Arrival Time**: Measures the time between consecutive packets.\n",
        "- **Flags**: Various TCP flags, such as `SYN`, `ACK`, and `RST`, are used to indicate specific network conditions.\n",
        "- **Active and Idle Times**: Represents the time intervals when the flow was actively transmitting data and when it was idle.\n",
        "\n",
        "### Dataset Usage\n",
        "The data can be used to detect and analyze network intrusions, specifically **brute-force attacks**. By examining patterns in traffic, such as sudden spikes in flow rates or repeated login attempts, it is possible to identify suspicious behaviors indicative of an ongoing attack.\n",
        "\n",
        "### Example Records\n",
        "Each row in the dataset represents a network flow, containing information like the flow's duration, the total number of packets, and the size of packets transmitted in both directions. The dataset also provides a label indicating whether the flow is normal (benign) or an attack.\n",
        "\n",
        "Overall, this dataset is useful for tasks such as detecting brute-force attacks and understanding the characteristics of network traffic during different types of events."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8AHMvby_siOX"
      },
      "source": [
        "## Question 1: DNN for Regression or Classification (20 points)\n",
        "\n",
        "### Building a Dense Neural Network (DNN) or CNN OR LSTM depending on dataset model with Keras, either for regression or classification\n",
        "\n",
        "We have chosen DNNs (Dense Neural Networks) as they are generally better suited to structured data where each feature contributes to the outcome independently, which is the case with our dataset as it has more of a tabular nature where the network-attack's data log records are independent of each other.\n",
        "\n",
        "We can also use CNNs which can be more effective if our data had  spatial or temporal patterns like the time in which the attacks take place in our data(time-series). But our data is structured for tabular analysis, where each record is independent of others. We would neeed to transform the data into a grid format that captures temporal or spatial dependencies to achieve this so we chose to stick to DNNs."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preprocessing\n",
        "We first started by preprocessing the data, we started by dealing with the infinity and NaN values in our dataset. so I replaced the infinite values that are outside of the float range with NaN and then used an imputer to replace the NaN values with averages."
      ],
      "metadata": {
        "id": "Nk79yP7p_Td7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "OQpapHU8uZAe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb23a93d-cfd6-48b3-81f5-bceb0811f7cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Non-numeric columns: Index(['Timestamp', 'Label'], dtype='object')\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Step 1: Drop Non-Numeric Columns\n",
        "# Confirm and drop non-numeric columns\n",
        "non_numeric_columns = df.select_dtypes(include=['object']).columns\n",
        "print(\"Non-numeric columns:\", non_numeric_columns)\n",
        "\n",
        "# Drop 'Timestamp' and any other non-numeric columns if present\n",
        "X = df.drop(columns=['Timestamp', 'Label'])  # Replace 'Timestamp' and 'Label' with any additional non-numeric columns\n",
        "y = df['Label']\n",
        "\n",
        "# Step 2: Handle Infinity Values\n",
        "# Replace infinity values in X with NaN\n",
        "X.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "\n",
        "# Step 3: Impute Missing Values\n",
        "# Replace NaN values with the column mean\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "X = pd.DataFrame(imputer.fit_transform(X), columns=X.columns)\n",
        "\n",
        "# Step 4: Train-Test Split with Stratification\n",
        "# Split the data into training and testing sets, maintaining class balance\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, stratify=y, random_state=123)\n",
        "\n",
        "# Reduce training set size if memory is an issue\n",
        "X_train, y_train = X_train[:100000], y_train[:100000]\n",
        "\n",
        "# Step 5: Feature Scaling\n",
        "# Standardize features for improved model performance\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Step 6: Encode Labels\n",
        "# Encode the target variable and convert it to a categorical format for multi-class classification\n",
        "encoder = LabelEncoder()\n",
        "y_train = encoder.fit_transform(y_train)\n",
        "y_test = encoder.transform(y_test)\n",
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "### Model 1: Baseline Dense Neural Network (DNN)\n",
        "\n",
        "**Description**: This model serves as a baseline, designed with a relatively simple structure to evaluate if a straightforward DNN can handle the dataset effectively.\n",
        "\n",
        "1. **Input Layer (Dense Layer)**:\n",
        "   - **64 units** with **ReLU activation**.\n",
        "   - This layer processes the input features and begins identifying patterns in the data.\n",
        "\n",
        "2. **Dropout Layer**:\n",
        "   - **Dropout rate of 0.5** to reduce overfitting.\n",
        "   - Regularization at this stage forces the model to generalize and prevents reliance on specific neurons.\n",
        "\n",
        "3. **Second Dense Layer**:\n",
        "   - **32 units** with **ReLU activation**.\n",
        "   - This layer further refines the features passed from the first layer, helping the model learn deeper representations of the data.\n",
        "\n",
        "4. **Second Dropout Layer**:\n",
        "   - **Dropout rate of 0.3** for additional regularization.\n",
        "   - This layer aims to further reduce overfitting, especially as the model learns more specific patterns in the data.\n",
        "\n",
        "5. **Output Layer**:\n",
        "   - **Softmax activation** with **units equal to the number of classes**.\n",
        "   - Provides the final class probabilities for multi-class classification.\n",
        "\n",
        "**Purpose**: This model is simple yet effective as a starting point. It allows us to gauge if a relatively shallow architecture can handle the dataset. Regularization through dropout helps reduce overfitting, ensuring the model is versatile enough for generalization.\n",
        "\n",
        "---\n",
        "\n",
        "### Model 2: Enhanced DNN with Increased Complexity\n",
        "\n",
        "**Description**: This model builds on Model 1 by increasing the number of layers and units, aiming to capture more complex patterns within the data.\n",
        "\n",
        "1. **Input Layer (Dense Layer)**:\n",
        "   - **128 units** with **ReLU activation**.\n",
        "   - This layer’s larger size helps the model capture a wider variety of patterns from the input data.\n",
        "\n",
        "2. **Dropout Layer**:\n",
        "   - **Dropout rate of 0.5** for regularization.\n",
        "\n",
        "3. **Second Dense Layer**:\n",
        "   - **64 units** with **ReLU activation**.\n",
        "   - This layer processes the refined features from the input layer and begins more complex feature extraction.\n",
        "\n",
        "4. **Second Dropout Layer**:\n",
        "   - **Dropout rate of 0.3**.\n",
        "\n",
        "5. **Third Dense Layer**:\n",
        "   - **32 units** with **ReLU activation**.\n",
        "   - An additional layer adds depth to the model, allowing it to capture intricate patterns that a simpler model might miss.\n",
        "\n",
        "6. **Third Dropout Layer**:\n",
        "   - **Dropout rate of 0.2** to regularize deeper layers.\n",
        "\n",
        "7. **Output Layer**:\n",
        "   - **Softmax activation** with **units equal to the number of classes**.\n",
        "\n",
        "**Purpose**: The increased complexity in Model 2 allows it to potentially capture more nuanced patterns within the dataset. The additional layer and units can help address any limitations observed in Model 1, especially if the data has subtle interactions that require more depth to learn. Dropout layers are applied strategically to each hidden layer to control overfitting as the model complexity increases.\n",
        "\n",
        "---\n",
        "\n",
        "### Model 3: Deep DNN with Minimal Dropout\n",
        "\n",
        "**Description**: This model further increases the depth by adding more layers but reduces the dropout rate to observe how regularization affects a deep model’s performance and generalization.\n",
        "\n",
        "1. **Input Layer (Dense Layer)**:\n",
        "   - **256 units** with **ReLU activation**.\n",
        "   - A large number of units in the input layer help capture a broad set of patterns from the features.\n",
        "\n",
        "2. **Second Dense Layer**:\n",
        "   - **128 units** with **ReLU activation**.\n",
        "   - Further processes complex patterns and interactions.\n",
        "\n",
        "3. **Dropout Layer**:\n",
        "   - **Dropout rate of 0.2**.\n",
        "   - Regularization is applied lightly to test if this deeper architecture can generalize without heavy dropout.\n",
        "\n",
        "4. **Third Dense Layer**:\n",
        "   - **64 units** with **ReLU activation**.\n",
        "\n",
        "5. **Fourth Dense Layer**:\n",
        "   - **32 units** with **ReLU activation**.\n",
        "   \n",
        "6. **Output Layer**:\n",
        "   - **Softmax activation** with **units equal to the number of classes**.\n",
        "\n",
        "**Purpose**: Model 3 has the most depth and units, aiming to fully leverage the dataset's features. This design tests if a more complex architecture, even with reduced dropout, can generalize well or if it becomes prone to overfitting. It allows us to observe how reduced regularization impacts the performance of a deep model.\n",
        "\n",
        "---\n",
        "\n",
        "### Comparison Rationale\n",
        "\n",
        "These three models were chosen to provide a spectrum of complexity:\n",
        "\n",
        "1. **Model 1 (Baseline)**: A simpler model that will serve as a control, helping to establish if a basic architecture is sufficient for accurate classification. We expect it to be quick to train with potentially reasonable accuracy, but it might miss out on more complex patterns.\n",
        "\n",
        "2. **Model 2 (Enhanced Complexity)**: By increasing the number of layers and units, Model 2 tests if a moderate increase in complexity provides better results without overfitting. We expect this model to perform better than Model 1 due to its ability to learn more intricate patterns, while still being controlled by dropout layers.\n",
        "\n",
        "3. **Model 3 (Deep DNN)**: The deepest model with minimal dropout explores if adding more layers and units is beneficial or if it introduces overfitting. We expect this model to potentially achieve high accuracy on the training set but might overfit if the dataset size isn’t large enough to support such a complex architecture.\n",
        "\n",
        "### Expected Observations and Hypotheses\n",
        "\n",
        "1. **Training and Validation Accuracy**:\n",
        "   - **Model 1** may have lower accuracy compared to Models 2 and 3, especially if the dataset has complex patterns that a shallow architecture can’t capture.\n",
        "   - **Model 2** should ideally have a higher accuracy than Model 1, balancing complexity and regularization.\n",
        "   - **Model 3** might show high training accuracy but could suffer in validation accuracy if it overfits due to insufficient regularization.\n",
        "\n",
        "2. **Overfitting**:\n",
        "   - We expect **Model 1** to have the least risk of overfitting due to its simplicity.\n",
        "   - **Model 2** should perform better at generalization due to the balanced complexity and regularization.\n",
        "   - **Model 3** might overfit due to reduced dropout, especially if the model complexity is too high for the data.\n",
        "\n",
        "3. **Generalization**:\n",
        "   - **Model 1** may have better generalization on simpler patterns, but might miss subtle distinctions.\n",
        "   - **Model 2** could generalize well, capturing both simple and complex patterns with appropriate dropout layers.\n",
        "   - **Model 3** could struggle with generalization if it overfits due to insufficient regularization, despite its high complexity.\n",
        "\n",
        "By comparing these three models, we can determine the optimal balance between model complexity and regularization for this dataset, identifying which architecture provides the best performance while minimizing overfitting. This approach will guide future architectural decisions for similar structured datasets."
      ],
      "metadata": {
        "id": "Auzwtxsi_k_R"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "migrBb161Ms-",
        "outputId": "97e6517d-d143-4344-816e-c47c8f1da125"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training Model 1...\n",
            "Epoch 1/50\n",
            "\u001b[1m19661/19661\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 2ms/step - accuracy: 0.9911 - loss: 0.0345 - val_accuracy: 0.9999 - val_loss: 6.3943e-04\n",
            "Epoch 2/50\n",
            "\u001b[1m 2953/19661\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m29s\u001b[0m 2ms/step - accuracy: 0.9998 - loss: 0.0018"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Encode labels if multi-class classification\n",
        "encoder = LabelEncoder()\n",
        "y = encoder.fit_transform(y)\n",
        "y = to_categorical(y)  # One-hot encode labels for Keras\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, stratify=y, random_state=123)\n",
        "\n",
        "# Normalize the features for better performance\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Define a function to build, compile, and train a model\n",
        "def build_and_train_model(layers, dropout_rates, input_shape, epochs=50, batch_size=32, model_name=\"Model\"):\n",
        "    model = Sequential()\n",
        "    for i, (units, dropout_rate) in enumerate(zip(layers, dropout_rates)):\n",
        "        if i == 0:\n",
        "            # Input layer with input shape defined\n",
        "            model.add(Dense(units, input_shape=(input_shape,), activation='relu'))\n",
        "        else:\n",
        "            # Hidden layers\n",
        "            model.add(Dense(units, activation='relu'))\n",
        "        model.add(Dropout(dropout_rate))\n",
        "    # Output layer\n",
        "    model.add(Dense(y_train.shape[1], activation='softmax'))  # Adjust for multi-class classification\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    # Train the model\n",
        "    print(f\"\\nTraining {model_name}...\")\n",
        "    history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_split=0.2, verbose=1)\n",
        "\n",
        "    # Evaluate the model\n",
        "    test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=1)\n",
        "    print(f\"{model_name} - Test Loss: {test_loss}, Test Accuracy: {test_accuracy}\")\n",
        "\n",
        "    return model, history\n",
        "\n",
        "# Model 1: Baseline Model\n",
        "layers_model_1 = [64, 32]\n",
        "dropout_rates_model_1 = [0.5, 0.3]\n",
        "model_1, history_1 = build_and_train_model(layers_model_1, dropout_rates_model_1, input_shape=X_train.shape[1], model_name=\"Model 1\")\n",
        "\n",
        "# Model 2: Enhanced Model with More Layers\n",
        "layers_model_2 = [128, 64, 32]\n",
        "dropout_rates_model_2 = [0.5, 0.3, 0.2]\n",
        "model_2, history_2 = build_and_train_model(layers_model_2, dropout_rates_model_2, input_shape=X_train.shape[1], model_name=\"Model 2\")\n",
        "\n",
        "# Model 3: Deep Model with Minimal Dropout\n",
        "layers_model_3 = [256, 128, 64, 32]\n",
        "dropout_rates_model_3 = [0.2, 0.2, 0.2, 0]\n",
        "model_3, history_3 = build_and_train_model(layers_model_3, dropout_rates_model_3, input_shape=X_train.shape[1], model_name=\"Model 3\")\n",
        "\n",
        "# Function to plot training and validation accuracy and loss\n",
        "def plot_history(history, model_name):\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    # Accuracy plot\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
        "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "    plt.title(f'{model_name} Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "\n",
        "    # Loss plot\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history.history['loss'], label='Train Loss')\n",
        "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "    plt.title(f'{model_name} Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# Plot training and validation accuracy and loss for each model\n",
        "plot_history(history_1, \"Model 1\")\n",
        "plot_history(history_2, \"Model 2\")\n",
        "plot_history(history_3, \"Model 3\")\n",
        "\n",
        "# Evaluation function to get confusion matrix and classification report\n",
        "def evaluate_model(model, X_test, y_test, model_name):\n",
        "    y_pred = np.argmax(model.predict(X_test), axis=1)\n",
        "    y_true = np.argmax(y_test, axis=1)\n",
        "\n",
        "    print(f\"\\n{model_name} Classification Report:\\n\", classification_report(y_true, y_pred))\n",
        "    print(f\"{model_name} Confusion Matrix:\\n\", confusion_matrix(y_true, y_pred))\n",
        "\n",
        "    # Plot confusion matrix as heatmap\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(confusion_matrix(y_true, y_pred), annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False)\n",
        "    plt.title(f\"{model_name} Confusion Matrix\")\n",
        "    plt.xlabel(\"Predicted Label\")\n",
        "    plt.ylabel(\"True Label\")\n",
        "    plt.show()\n",
        "\n",
        "# Evaluate each model\n",
        "evaluate_model(model_1, X_test, y_test, \"Model 1\")\n",
        "evaluate_model(model_2, X_test, y_test, \"Model 2\")\n",
        "evaluate_model(model_3, X_test, y_test, \"Model 3\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z4DKI9ne068R"
      },
      "source": [
        "## Evaluating Model Performance\n",
        "Getting model performance using sklearn tools\n",
        "Why have we chosen a classification report and confusion matrix?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "czRkfkSR05IR"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
        "print(f\"Test Accuracy: {test_accuracy}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualizing Results (Training and Validation)\n",
        "Using Matplotlib to plot training validation accuracy and loss values"
      ],
      "metadata": {
        "id": "qzpF9hF89w2n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot training & validation accuracy values\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Model accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Djp-uzJtt8Qx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot training & validation loss values\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "mmy9zOwht77B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = np.argmax(model.predict(X_test), axis=1)\n",
        "y_true = np.argmax(y_test, axis=1)\n",
        "\n",
        "print(\"Classification Report:\\n\", classification_report(y_true, y_pred))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_true, y_pred))"
      ],
      "metadata": {
        "id": "hXc4ENv-uKjB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Generate the confusion matrix heatmap\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(confusion_matrix(y_true, y_pred), annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False)\n",
        "plt.title(\"Confusion Matrix Heatmap\")\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "hSHdme-s-b7q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Discussion and Analysis\n",
        "The Dense Neural Network achieved nearly perfect accuracy on the test data, with both training and validation accuracies close to 100%. The classification report and confusion matrix indicate that the model has learned to classify all three classes accurately, with precision, recall, and F1-scores all at 1.00 across the board.\n",
        "\n",
        "### Observations:\n",
        "- **High Accuracy**: The DNN model shows excellent performance with no significant misclassification across all classes.\n",
        "- **Comparison with Assignment 2**: Compared to traditional machine learning models in Assignment 2, the DNN seems to have learned complex patterns in the network traffic data, which might be why it achieves such high accuracy.\n",
        "- **Potential Overfitting**: The validation accuracy is very close to the training accuracy, which suggests that the model generalizes well to the test data, though there is still some risk of overfitting given the perfect scores. In a real-world scenario, further regularization or testing on an unseen dataset would be advisable.\n"
      ],
      "metadata": {
        "id": "qeXHpdrb-RBB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion\n",
        "In this assignment, a Dense Neural Network (DNN) was trained on network traffic data to classify different types of network activities, including brute-force attacks. The data was preprocessed by handling NaN and infinity values, then standardized before being fed into the model. The DNN achieved excellent results, with perfect accuracy on both training and test data, indicating it is highly effective for this classification task.\n",
        "\n",
        "### Key Takeaways:\n",
        "- DNNs demonstrate a strong ability to model complex, high-dimensional data, making them a suitable choice for network intrusion detection.\n",
        "- The model's high accuracy reflects its ability to capture patterns that differentiate between benign and attack network flows.\n",
        "- Compared to traditional machine learning methods, DNNs may offer improved performance for tasks requiring deep representation learning.\n",
        "\n",
        "In summary, this assignment illustrates the effectiveness of DNNs in structured, tabular data applications, particularly when feature engineering and preprocessing are aligned with model requirements.\n"
      ],
      "metadata": {
        "id": "Bq_YUZgE-Xzx"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}